{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d049857",
   "metadata": {},
   "source": [
    "## **Step 1: Code to Load and Featurize Both Datasets**\n",
    "First, we need a function to load the raw accelerometer data and another to extract features using tsfel. We'll apply this to the UCI training data and your custom test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875c5876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing UCI HAR dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanamanthagouda\\AppData\\Local\\Temp\\ipykernel_41700\\270265782.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signals = [pd.read_csv(os.path.join(signals_path, f), header=None, delim_whitespace=True) for f in signal_files]\n",
      "C:\\Users\\Hanamanthagouda\\AppData\\Local\\Temp\\ipykernel_41700\\270265782.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signals = [pd.read_csv(os.path.join(signals_path, f), header=None, delim_whitespace=True) for f in signal_files]\n",
      "C:\\Users\\Hanamanthagouda\\AppData\\Local\\Temp\\ipykernel_41700\\270265782.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signals = [pd.read_csv(os.path.join(signals_path, f), header=None, delim_whitespace=True) for f in signal_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI data processed. Feature matrix shape: (7352, 468)\n",
      "\n",
      "Processing custom dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['ax', 'ay', 'az']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 168\u001b[0m\n\u001b[0;32m    165\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 121\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# 2. Process Custom Test Data\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing custom dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m my_raw_list, y_test_my_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_my_raw_data_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMY_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m my_raw_list:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAborting: Custom data loading failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 71\u001b[0m, in \u001b[0;36mload_my_raw_data_custom\u001b[1;34m(base_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label:\n\u001b[0;32m     70\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, file_name)\n\u001b[1;32m---> 71\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43max\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43may\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     73\u001b[0m     labels_list\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hanamanthagouda\\Desktop\\Acads\\ML\\ML_Assignment1\\es335\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:988\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    986\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m     )\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['ax', 'ay', 'az']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tsfel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the base paths for your custom data and the UCI dataset.\n",
    "MY_DATA_PATH = 'data/myData'\n",
    "UCI_DATA_PATH = 'data/UCI_data/train'\n",
    "\n",
    "# --- Data Loading Functions ---\n",
    "\n",
    "def load_uci_raw_data(base_path):\n",
    "    \"\"\"\n",
    "    Loads raw inertial signals and labels from the UCI HAR dataset structure.\n",
    "    \"\"\"\n",
    "    signals_path = os.path.join(base_path, 'Inertial Signals/')\n",
    "    signal_files = ['total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "    try:\n",
    "        y = pd.read_csv(os.path.join(base_path, 'y_train.txt'), header=None).iloc[:, 0]\n",
    "        signals = [pd.read_csv(os.path.join(signals_path, f), header=None, delim_whitespace=True) for f in signal_files]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find UCI data file - {e}.\")\n",
    "        return [], None\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(len(signals[0])):\n",
    "        df = pd.DataFrame({\n",
    "            'ax': signals[0].iloc[i],\n",
    "            'ay': signals[1].iloc[i],\n",
    "            'az': signals[2].iloc[i]\n",
    "        })\n",
    "        data_list.append(df)\n",
    "            \n",
    "    return data_list, y\n",
    "\n",
    "def load_my_raw_data_custom(base_path):\n",
    "    \"\"\"\n",
    "    Loads and parses custom accelerometer data from a flat directory\n",
    "    based on a '{participant}_{activity}.csv' naming convention.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    activity_map = {\n",
    "        'sit': 'SITTING',\n",
    "        'sleep': 'LAYING',\n",
    "        'stand': 'STANDING',\n",
    "        'walk': 'WALKING',\n",
    "        'walkd': 'WALKING_DOWNSTAIRS',\n",
    "        'walku': 'WALKING_UPSTAIRS'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Error: The directory '{base_path}' does not exist.\")\n",
    "        return [], None\n",
    "        \n",
    "    for file_name in os.listdir(base_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            try:\n",
    "                activity_key = file_name.split('_')[1].split('.')[0]\n",
    "                label = activity_map.get(activity_key)\n",
    "                \n",
    "                if label:\n",
    "                    file_path = os.path.join(base_path, file_name)\n",
    "                    df = pd.read_csv(file_path, usecols=['ax', 'ay', 'az'])\n",
    "                    data_list.append(df)\n",
    "                    labels_list.append(label)\n",
    "            except IndexError:\n",
    "                # This handles files that don't match the expected naming pattern.\n",
    "                pass\n",
    "\n",
    "    if not data_list:\n",
    "        print(\"Error: No valid CSV data could be loaded. Verify file names and content.\")\n",
    "\n",
    "    return data_list, pd.Series(labels_list)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "def extract_tsfel_features(data_list):\n",
    "    \"\"\"\n",
    "    Extracts a comprehensive set of time-series features using TSFEL.\n",
    "    Runs silently without progress bars.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    features_list = [tsfel.time_series_features_extractor(cfg, df, fs=50, verbose=0) for df in data_list]\n",
    "    return pd.concat(features_list, ignore_index=True)\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data loading, feature extraction,\n",
    "    model training, and evaluation pipeline.\n",
    "    \"\"\"\n",
    "    # 1. Process UCI Training Data\n",
    "    print(\"Processing UCI HAR dataset...\")\n",
    "    uci_raw_list, y_train_uci_raw = load_uci_raw_data(UCI_DATA_PATH)\n",
    "    if not uci_raw_list:\n",
    "        print(\"Aborting: UCI data loading failed.\")\n",
    "        return\n",
    "\n",
    "    X_train_uci_tsfel = extract_tsfel_features(uci_raw_list)\n",
    "    uci_activity_labels = {\n",
    "        1: 'WALKING', 2: 'WALKING_UPSTAIRS', 3: 'WALKING_DOWNSTAIRS', \n",
    "        4: 'SITTING', 5: 'STANDING', 6: 'LAYING'\n",
    "    }\n",
    "    y_train_uci_tsfel = y_train_uci_raw.map(uci_activity_labels)\n",
    "    print(f\"UCI data processed. Feature matrix shape: {X_train_uci_tsfel.shape}\")\n",
    "\n",
    "    # 2. Process Custom Test Data\n",
    "    print(\"\\nProcessing custom dataset...\")\n",
    "    my_raw_list, y_test_my_data = load_my_raw_data_custom(MY_DATA_PATH)\n",
    "    if not my_raw_list:\n",
    "        print(\"Aborting: Custom data loading failed.\")\n",
    "        return\n",
    "\n",
    "    X_test_my_data_tsfel = extract_tsfel_features(my_raw_list)\n",
    "    print(f\"Custom data processed. Feature matrix shape: {X_test_my_data_tsfel.shape}\")\n",
    "\n",
    "    # 3. Align Features, Train Model, and Predict\n",
    "    print(\"\\nTraining model and performing prediction...\")\n",
    "    \n",
    "    # Sanitize column names for model compatibility.\n",
    "    X_train_uci_tsfel.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_uci_tsfel.columns]\n",
    "    X_test_my_data_tsfel.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_my_data_tsfel.columns]\n",
    "    \n",
    "    # Use only the features present in both datasets.\n",
    "    common_features = X_train_uci_tsfel.columns.intersection(X_test_my_data_tsfel.columns)\n",
    "    X_train = X_train_uci_tsfel[common_features]\n",
    "    X_test = X_test_my_data_tsfel[common_features]\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "    model.fit(X_train, y_train_uci_tsfel)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 4. Evaluate and Report Performance\n",
    "    print(\"\\n--- Model Performance on Custom Data ---\")\n",
    "    accuracy = accuracy_score(y_test_my_data, y_pred)\n",
    "    report = classification_report(y_test_my_data, y_pred, zero_division=0)\n",
    "    \n",
    "    all_labels = sorted(list(set(y_test_my_data) | set(y_pred)))\n",
    "    cm = confusion_matrix(y_test_my_data, y_pred, labels=all_labels)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot the confusion matrix for visual analysis.\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=all_labels, yticklabels=all_labels)\n",
    "    plt.title('Confusion Matrix: UCI-Trained (TSFEL) Model on Your Data')\n",
    "    plt.ylabel('Actual (Your Data)')\n",
    "    plt.xlabel('Predicted by Model')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358e494",
   "metadata": {},
   "source": [
    "## **Step 2: Code to Train the Model and Predict**\n",
    "Now that both datasets have the same TSFEL features, we can train on the UCI data and test on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf33dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and Evaluate the Model ---\n",
    "\n",
    "# Ensure both dataframes have the same columns\n",
    "common_features = X_train_uci_tsfel.columns.intersection(X_test_my_data_tsfel.columns)\n",
    "X_train = X_train_uci_tsfel[common_features]\n",
    "X_test = X_test_my_data_tsfel[common_features]\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "model.fit(X_train, y_train_uci_tsfel)\n",
    "\n",
    "# Predict on your featurized data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Report the Results ---\n",
    "accuracy = accuracy_score(y_test_my_data, y_pred)\n",
    "report = classification_report(y_test_my_data, y_pred)\n",
    "cm = confusion_matrix(y_test_my_data, y_pred, labels=model.classes_)\n",
    "\n",
    "print(\"\\n--- TSFEL Model Performance on Your Data ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.title('Confusion Matrix: UCI-Trained (TSFEL) Model on Your Data')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534028c",
   "metadata": {},
   "source": [
    "### **Why Not Authors features** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d832410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Author-Provided UCI Feature Data ---\n",
    "X_train_author = pd.read_csv('data/UCI_data/train/X_train.txt', header=None, delim_whitespace=True)\n",
    "y_train_author_raw = pd.read_csv('data/UCI_data/train/y_train.txt', header=None, squeeze=True)\n",
    "y_train_author = y_train_author_raw.map(activity_labels)\n",
    "\n",
    "# Train the model\n",
    "author_model = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "author_model.fit(X_train_author, y_train_author)\n",
    "\n",
    "print(\"Model trained on author-provided UCI features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cbbf9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
